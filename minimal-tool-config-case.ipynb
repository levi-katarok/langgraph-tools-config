{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Reproduction of Passing Config To Tools In Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in ./env/lib/python3.11/site-packages (0.2.11)\n",
      "Requirement already satisfied: langchain-core in ./env/lib/python3.11/site-packages (0.2.34)\n",
      "Requirement already satisfied: typing-extensions in ./env/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: langchain-openai in ./env/lib/python3.11/site-packages (0.1.22)\n",
      "Requirement already satisfied: python-dotenv in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<2.0.0,>=1.0.2 in ./env/lib/python3.11/site-packages (from langgraph) (1.0.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.11/site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.11/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in ./env/lib/python3.11/site-packages (from langchain-core) (0.1.101)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./env/lib/python3.11/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./env/lib/python3.11/site-packages (from langchain-core) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./env/lib/python3.11/site-packages (from langchain-core) (8.5.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in ./env/lib/python3.11/site-packages (from langchain-openai) (1.42.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./env/lib/python3.11/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (3.10.7)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core) (2.32.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./env/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./env/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.5.0)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./env/lib/python3.11/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./env/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core) (2.20.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.7.24)\n",
      "Requirement already satisfied: idna>=2.8 in ./env/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.75->langchain-core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core) (2.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph langchain-core typing-extensions langchain-openai python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal, Optional, Callable\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph import MessagesState\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dialog_stack(left: list[str], right: Optional[str]) -> list[str]:\n",
    "    \"\"\"Push or pop the state\"\"\"\n",
    "    if right is None:\n",
    "        return left\n",
    "    if right == \"pop\":\n",
    "        return left[:-1]\n",
    "    return left + [right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entry_node(assistant_name: str, new_dialog_state: str) -> Callable:\n",
    "    def entry_node(state: MessagesState) -> dict:\n",
    "        tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"The assistant is now the {assistant_name}. Reflect on the above conversation between the host assistant and the user. \"  # noqa\n",
    "                    f\"Look at your provided tools to assist the user. Remember, you are {assistant_name},\"\n",
    "                    \"If the user changes their mind or needs help which you can't provide with your tools, call the CompleteOrEscalate function to let the primary host assistant take control.\"  # noqa\n",
    "                    \"Act as the proxy for the assistant.\",\n",
    "                    tool_call_id=tool_call_id,\n",
    "                )\n",
    "            ],\n",
    "            \"dialog_state\": new_dialog_state,\n",
    "        }\n",
    "\n",
    "    return entry_node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: MessagesState, config: RunnableConfig):\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)\n",
    "            if not result.tool_calls and (not result.content or isinstance(result.content, list) and not result.content[0].get(\"text\")):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "class CompleteOrEscalate(BaseModel):\n",
    "    \"\"\"A tool marks the current task as completed and/or to escalate control of the dialog to the main assistant,\n",
    "    who can re-route the dialog based on the user's needs\"\"\"\n",
    "\n",
    "    cancel: bool = True\n",
    "    reason: str\n",
    "\n",
    "    class Config:\n",
    "        scheme_extra = {\n",
    "            \"example\": {\"cancel\": True, \"reason\": \"User changed their mind about the current task\"},\n",
    "            \"example_1\": {\"cancel\": True, \"reason\": \"I have fully completed the task\"},\n",
    "            \"example_2\": {\"cancel\": False, \"reason\": \"I have not fully completed the task\"},\n",
    "        }\n",
    "\n",
    "\n",
    "# Booking Assistant\n",
    "class BookingAssistant(BaseModel):\n",
    "    \"\"\"The Booking Assistant assists manages bookings\"\"\"  # noqa\n",
    "\n",
    "    request: str = Field(description=\"Any necessary followup questions the booking assistant should clarify before proceeding.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_assistant_runnable():\n",
    "    model_name = \"gpt-4o\"\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "\n",
    "    primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You a primary personal assistant.\"\n",
    "                \"Delegate the task to the appropriate specialized assistant by invoking the corresponding tool. \"\n",
    "                \"You are not able to make these types of changes yourself.\"\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "    primary_assistant_tools = []\n",
    "    assistant_runnable = primary_assistant_prompt | llm.bind_tools([BookingAssistant])\n",
    "    return assistant_runnable, primary_assistant_tools\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks([RunnableLambda(handle_tool_error)], exception_key=\"error\")\n",
    "\n",
    "\n",
    "def pop_dialog_state(state: MessagesState) -> dict:\n",
    "    messages = []\n",
    "    if state[\"messages\"][-1].tool_calls:\n",
    "        messages.append(\n",
    "            ToolMessage(\n",
    "                content=\"Resuming dialog with the host assistant. Please reflect on the past conversation and assist the user as needed.\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        )\n",
    "    return {\n",
    "        \"dialog_state\": \"pop\",\n",
    "        \"messages\": messages,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Booking Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "\n",
    "@tool\n",
    "def query_bookings(config: RunnableConfig) -> str:\n",
    "    \"\"\"Query only the current user's bookings\"\"\"\n",
    "    \n",
    "    configuration = config.get(\"configurable\", {})\n",
    "    print('Configuration:', configuration)\n",
    "    \n",
    "    user_id = configuration.get(\"user_id\", None)\n",
    "    print(\"User ID:\", user_id)\n",
    "    \n",
    "    if not user_id:\n",
    "        return \"Failed to query bookings. No User ID configured. Please report back to the user\"\n",
    "    else: \n",
    "        return \"You have 5 bookings next week.\"\n",
    "\n",
    "\n",
    "def get_schedule_assistant_runnable():\n",
    "    schedule_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful assistant that helps users make changes or query to their current or historical bookings.\"\n",
    "            ),\n",
    "            (\"placeholder\", \"{messages}\"),\n",
    "        ]\n",
    "    )\n",
    "    model_name = \"gpt-4o\"\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    booking_safe_tools = [query_bookings]\n",
    "    booking_runnable = schedule_assistant_prompt | llm.bind_tools(booking_safe_tools + [CompleteOrEscalate])\n",
    "    return booking_safe_tools, booking_runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph() -> StateGraph:\n",
    "\n",
    "    builder = StateGraph(MessagesState)\n",
    "\n",
    "    builder.set_entry_point(\"primary_assistant\")\n",
    "    booking_safe_tools, booking_runnable = get_schedule_assistant_runnable()\n",
    "    builder.add_node(\n",
    "        \"enter_booking_assistant\",\n",
    "        create_entry_node(\"Booking Assistant\", \"booking_assistant\"),\n",
    "    )\n",
    "    builder.add_node(\"booking_assistant\", Assistant(booking_runnable))\n",
    "    builder.add_edge(\"enter_booking_assistant\", \"booking_assistant\")\n",
    "    builder.add_node(\"booking_safe_tools\", create_tool_node_with_fallback(booking_safe_tools))\n",
    "\n",
    "    def route_booking_assistant(\n",
    "        state: MessagesState,\n",
    "    ) -> Literal[\"booking_safe_tools\", \"leave_skill\", \"__end__\"]:\n",
    "        route = tools_condition(state)\n",
    "        if route == END:\n",
    "            return END\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        did_cancel = any(tc[\"name\"] == CompleteOrEscalate.__name__ for tc in tool_calls)\n",
    "        if did_cancel:\n",
    "            return \"leave_skill\"\n",
    "        safe_toolnames = [t.name for t in booking_safe_tools]\n",
    "        if all(tc[\"name\"] in safe_toolnames for tc in tool_calls):\n",
    "            return \"booking_safe_tools\"\n",
    "\n",
    "    builder.add_edge(\"booking_safe_tools\", \"booking_assistant\")\n",
    "    builder.add_conditional_edges(\"booking_assistant\", route_booking_assistant)\n",
    "\n",
    "    # End Query Work Hours And Task Assistant\n",
    "\n",
    "    builder.add_node(\"leave_skill\", pop_dialog_state)\n",
    "    builder.add_edge(\"leave_skill\", \"primary_assistant\")\n",
    "\n",
    "    assistant_runnable, primary_assistant_tools = get_assistant_runnable()\n",
    "    builder.add_node(\"primary_assistant\", Assistant(assistant_runnable))\n",
    "    builder.add_node(\"primary_assistant_tools\", create_tool_node_with_fallback(primary_assistant_tools))\n",
    "\n",
    "    def route_primary_assistant(\n",
    "        state: MessagesState,\n",
    "    ) -> Literal[\"primary_assistant_tools\", \"enter_booking_assistant\", \"__end__\"]:\n",
    "        route = tools_condition(state)\n",
    "        if route == END:\n",
    "            return END\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        if tool_calls:\n",
    "            if tool_calls[0][\"name\"] == BookingAssistant.__name__:\n",
    "                return \"enter_booking_assistant\"\n",
    "            return \"primary_assistant_tools\"\n",
    "        raise ValueError(\"Invalid Route\")\n",
    "\n",
    "    builder.add_conditional_edges(\n",
    "        \"primary_assistant\",\n",
    "        route_primary_assistant,\n",
    "        {\n",
    "            \"enter_booking_assistant\": \"enter_booking_assistant\",\n",
    "            \"primary_assistant_tools\": \"primary_assistant_tools\",\n",
    "            END: END,\n",
    "        },\n",
    "    )\n",
    "    builder.add_edge(\"primary_assistant_tools\", \"primary_assistant\")\n",
    "    return builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = build_graph()\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_event(llm_event: dict, _printed: set, max_length=1500):\n",
    "    current_state = llm_event.get(\"dialog_state\")\n",
    "    if current_state:\n",
    "        print(\"Currently in: \", current_state[-1])\n",
    "    message = llm_event.get(\"messages\")\n",
    "    if message:\n",
    "        if isinstance(message, list):\n",
    "            message = message[-1]\n",
    "        if message.id not in _printed:\n",
    "            msg_repr = message.pretty_repr(html=True)\n",
    "            if len(msg_repr) > max_length:\n",
    "                msg_repr = msg_repr[:max_length] + \" ... (truncated)\"\n",
    "            print(msg_repr)\n",
    "            _printed.add(message.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What shifts do I have this week? \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  BookingAssistant (call_I7o5ijMXKx8sWk0Xe3Kkdt8c)\n",
      " Call ID: call_I7o5ijMXKx8sWk0Xe3Kkdt8c\n",
      "  Args:\n",
      "    request: Can you provide the shift schedule for this week?\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "The assistant is now the Booking Assistant. Reflect on the above conversation between the host assistant and the user. Look at your provided tools to assist the user. Remember, you are Booking Assistant,If the user changes their mind or needs help which you can't provide with your tools, call the CompleteOrEscalate function to let the primary host assistant take control.Act as the proxy for the assistant.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  query_bookings (call_ew3Mx0mY0ebMbf34JPvsbhZR)\n",
      " Call ID: call_ew3Mx0mY0ebMbf34JPvsbhZR\n",
      "  Args:\n",
      "Configuration: {'user_id': 2222, '__pregel_task_id': '8091bdba-fe42-53d8-b1c4-9156b8287e9f', '__pregel_send': functools.partial(<function local_write at 0x11182ccc0>, <built-in method extend of collections.deque object at 0x111cfa5c0>, {'__start__': PregelNode(config={'tags': ['langsmith:hidden'], 'metadata': {}, 'configurable': {}}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), ChannelWrite<start:primary_assistant>(recurse=True, writes=[ChannelWriteEntry(channel='start:primary_assistant', value='__start__', skip_none=False, mapper=None)], require_at_least_one_of=None)]), 'enter_booking_assistant': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['branch:primary_assistant:route_primary_assistant:enter_booking_assistant'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<enter_booking_assistant,messages>(recurse=True, writes=[ChannelWriteEntry(channel='enter_booking_assistant', value='enter_booking_assistant', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]), 'booking_assistant': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['booking_safe_tools', 'enter_booking_assistant'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<booking_assistant,messages>(recurse=True, writes=[ChannelWriteEntry(channel='booking_assistant', value='booking_assistant', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), _route(recurse=True, _is_channel_writer=True)]), 'booking_safe_tools': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['branch:booking_assistant:route_booking_assistant:booking_safe_tools'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<booking_safe_tools,messages>(recurse=True, writes=[ChannelWriteEntry(channel='booking_safe_tools', value='booking_safe_tools', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]), 'leave_skill': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['branch:booking_assistant:route_booking_assistant:leave_skill'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<leave_skill,messages>(recurse=True, writes=[ChannelWriteEntry(channel='leave_skill', value='leave_skill', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]), 'primary_assistant': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['start:primary_assistant', 'leave_skill', 'primary_assistant_tools'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<primary_assistant,messages>(recurse=True, writes=[ChannelWriteEntry(channel='primary_assistant', value='primary_assistant', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']), _route(recurse=True, _is_channel_writer=True)]), 'primary_assistant_tools': PregelNode(config={'tags': [], 'metadata': {}, 'configurable': {}}, channels={'messages': 'messages'}, triggers=['branch:primary_assistant:route_primary_assistant:primary_assistant_tools'], mapper=functools.partial(<function _coerce_state at 0x11186f100>, <class 'langgraph.graph.message.MessagesState'>), writers=[ChannelWrite<primary_assistant_tools,messages>(recurse=True, writes=[ChannelWriteEntry(channel='primary_assistant_tools', value='primary_assistant_tools', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x104c5e140>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])])}, {'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x112de5350>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de7110>, 'enter_booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de4750>, 'booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de4250>, 'booking_safe_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc00d0>, 'leave_skill': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc18d0>, 'primary_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc1950>, 'primary_assistant_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc0510>, 'start:primary_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc1490>, 'branch:booking_assistant:route_booking_assistant:booking_safe_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x111a2d210>, 'branch:booking_assistant:route_booking_assistant:leave_skill': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd5010>, 'branch:primary_assistant:route_primary_assistant:enter_booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd5210>, 'branch:primary_assistant:route_primary_assistant:primary_assistant_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd4590>}, {}), '__pregel_read': functools.partial(<function local_read at 0x11182cc20>, {'v': 1, 'ts': '2024-08-22T16:29:53.389378+00:00', 'id': '1ef60a3c-2ae0-6026-bffe-026a38c7b370', 'channel_values': {}, 'channel_versions': {'__start__': 2, 'messages': 5, 'start:primary_assistant': 3, 'primary_assistant': 4, 'branch:primary_assistant:route_primary_assistant:enter_booking_assistant': 4, 'enter_booking_assistant': 5, 'booking_assistant': 5, 'branch:booking_assistant:route_booking_assistant:booking_safe_tools': 5}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': 1}, 'primary_assistant': {'start:primary_assistant': 2}, 'enter_booking_assistant': {'branch:primary_assistant:route_primary_assistant:enter_booking_assistant': 3}, 'booking_assistant': {'enter_booking_assistant': 4}}, 'pending_sends': [], 'current_tasks': {}}, {'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x112de5350>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de7110>, 'enter_booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de4750>, 'booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112de4250>, 'booking_safe_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc00d0>, 'leave_skill': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc18d0>, 'primary_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc1950>, 'primary_assistant_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc0510>, 'start:primary_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dc1490>, 'branch:booking_assistant:route_booking_assistant:booking_safe_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x111a2d210>, 'branch:booking_assistant:route_booking_assistant:leave_skill': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd5010>, 'branch:primary_assistant:route_primary_assistant:enter_booking_assistant': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd5210>, 'branch:primary_assistant:route_primary_assistant:primary_assistant_tools': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x112dd4590>}, PregelTaskWrites(name='booking_safe_tools', writes=deque([]), triggers=['branch:booking_assistant:route_booking_assistant:booking_safe_tools']), {'tags': [], 'metadata': {'user_id': 2222}, 'callbacks': None, 'recursion_limit': 25, 'configurable': {'user_id': 2222}}), '__pregel_checkpointer': None, '__pregel_resuming': False, 'checkpoint_id': '1ef60a3c-2ae0-6026-bffe-026a38c7b370', 'checkpoint_ns': 'booking_safe_tools'}\n",
      "User ID: 2222\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: query_bookings\n",
      "\n",
      "You have 5 bookings next week.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You have 5 shifts scheduled for next week. If you need more details about each shift, please let me know!\n"
     ]
    }
   ],
   "source": [
    "_printed = set()\n",
    "message = \"What shifts do I have this week? \"\n",
    "response_messages = []\n",
    "config = {\"configurable\": {\"user_id\": 2222}}\n",
    "\n",
    "llm_events = graph.stream({\"messages\": (\"user\", message)}, config, stream_mode=\"values\")\n",
    "for llm_event in llm_events:\n",
    "    _print_event(llm_event, _printed)\n",
    "    response_messages.append(llm_event)\n",
    "    if \"messages\" in llm_event and llm_event[\"messages\"]:\n",
    "        last_message = llm_event[\"messages\"][-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
